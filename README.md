# Telecom Churn Prediction MLOps Pipeline

## ğŸ“Œ Project Overview
This repository implements an end-to-end MLOps workflow for predicting telecom customer churn. The project combines robust data preprocessing, automated training and evaluation, experiment tracking, containerised deployment, continuous integration, and operational monitoring so that data scientists and DevOps engineers can collaborate on a production-ready churn prediction service.

## ğŸ§± Repository Structure
- `model_pipeline.py` / `model_pipeline1.py` â€“ Feature engineering, model training, evaluation utilities, and model persistence helpers used throughout the workflow.ã€F:model_pipeline.pyâ€ L1-L88ã€‘ã€F:model_pipeline1.pyâ€ L1-L92ã€‘
- `main.py` â€“ Lightweight script that prepares the data, trains a model, and logs metrics/models to MLflow for quick experimentation.ã€F:main.pyâ€ L1-L31ã€‘
- `main1.py` â€“ Production-grade entry point orchestrated by Jenkins that trains or evaluates the model, logs artefacts to MLflow, and forwards metrics to Elasticsearch.ã€F:main1.pyâ€ L1-L85ã€‘
- `app.py` â€“ FastAPI service exposing prediction, health-check, and retraining endpoints backed by the persisted churn model.ã€F:app.pyâ€ L1-L56ã€‘
- `docker-compose.yml` â€“ Multi-service stack for MLflow tracking, PostgreSQL storage, Elasticsearch/Kibana observability, and the model-serving API.ã€F:docker-compose.ymlâ€ L1-L52ã€‘
- `jenkinsfile` â€“ CI/CD pipeline that provisions dependencies, runs tests, trains the model, validates outputs, restarts the serving container, and archives artefacts.ã€F:jenkinsfileâ€ L1-L206ã€‘
- `Makefile` â€“ Automation entry points for installing dependencies, running the pipeline, launching the API, starting MLflow, and building Docker images.ã€F:Makefileâ€ L1-L62ã€‘
- `test_pipeline.py` â€“ Pytest-based unit tests covering data preparation, model training, evaluation, and persistence workflows.ã€F:test_pipeline.pyâ€ L1-L38ã€‘

## ğŸ”§ Key Tools and How They Are Used
| Tool | Role in the Pipeline | Why It Matters |
| ---- | -------------------- | -------------- |
| **Python 3 & scikit-learn** | Powers the RandomForest churn model plus preprocessing logic within the training scripts.ã€F:model_pipeline.pyâ€ L1-L88ã€‘ã€F:model_pipeline1.pyâ€ L1-L92ã€‘ | Offers a mature ML ecosystem with reliable tabular modelling capabilities.
| **Pandas & NumPy** | Handle CSV ingestion, feature manipulation, and numerical arrays across the pipeline scripts.ã€F:model_pipeline1.pyâ€ L1-L48ã€‘ | Enable expressive data wrangling and vectorised computation for telecom metrics.
| **MLflow** | Tracks experiments, parameters, metrics, and serialised models in both experimentation (`main.py`) and production runs (`main1.py`).ã€F:main.pyâ€ L1-L31ã€‘ã€F:main1.pyâ€ L36-L84ã€‘ | Ensures reproducibility, experiment comparison, and centralised model registry.
| **Joblib** | Saves and loads trained RandomForest models for reuse by the API and evaluation workflows.ã€F:model_pipeline.pyâ€ L68-L88ã€‘ã€F:model_pipeline1.pyâ€ L70-L92ã€‘ | Provides fast serialisation for scikit-learn estimators.
| **FastAPI** | Serves real-time predictions, health checks, and retraining endpoints through `app.py`.ã€F:app.pyâ€ L1-L56ã€‘ | Enables high-performance asynchronous APIs suitable for deployment.
| **Docker & docker-compose** | Bundle the application, MLflow UI, PostgreSQL, Elasticsearch, and Kibana into reproducible containers for local or CI execution.ã€F:docker-compose.ymlâ€ L1-L52ã€‘ | Guarantees consistent runtime environments across teams and stages.
| **Jenkins** | Automates CI/CD: dependency setup, testing, training, validation, service restarts, and artefact archiving via `jenkinsfile`.ã€F:jenkinsfileâ€ L1-L206ã€‘ | Provides reliable build automation and deployment governance.
| **Elasticsearch & Kibana** | Receive metrics logged from `main1.py` and expose them through Kibana dashboards for monitoring model health.ã€F:main1.pyâ€ L24-L85ã€‘ã€F:docker-compose.ymlâ€ L4-L52ã€‘ | Deliver observability for tracking accuracy, precision, recall, and F1 over time.
| **PostgreSQL** | Acts as the backing store for the serving layer to persist predictions or metadata when required by the Flask/FastAPI service.ã€F:docker-compose.ymlâ€ L27-L39ã€‘ | Supplies durable storage for downstream analytics and auditing.
| **Pytest** | Validates data preparation, training, evaluation, and model persistence routines to catch regressions early.ã€F:test_pipeline.pyâ€ L1-L38ã€‘ | Supports automated, repeatable testing integral to CI pipelines.

## ğŸ”„ Pipeline Walkthrough
1. **Data Ingestion & Preprocessing**  
   `prepare_data` merges training and test datasets, removes uninformative columns, encodes categorical variables, normalises numeric features, and optionally filters outliers to ensure clean inputs for modelling.ã€F:model_pipeline.pyâ€ L17-L66ã€‘ã€F:model_pipeline1.pyâ€ L11-L52ã€‘

2. **Model Training**  
   `train_model` fits a `RandomForestClassifier` on the prepared feature set, chosen for its robustness to feature scaling and interpretability.ã€F:model_pipeline.pyâ€ L68-L74ã€‘ã€F:model_pipeline1.pyâ€ L54-L70ã€‘

3. **Model Evaluation**  
   `evaluate_model` generates accuracy, precision, recall, and F1 metrics that are logged to MLflow and Elasticsearch for tracking and monitoring.ã€F:model_pipeline.pyâ€ L76-L86ã€‘ã€F:main1.pyâ€ L56-L85ã€‘

4. **Experiment Tracking**  
   `main.py` and `main1.py` register parameters, metrics, models, and an input example with MLflow so experiments remain reproducible and shareable.ã€F:main.pyâ€ L11-L31ã€‘ã€F:main1.pyâ€ L36-L84ã€‘

5. **Model Persistence & Serving**  
   Trained models are persisted via Joblib and consumed by the FastAPI application (`app.py`) to service prediction, health, and retraining requests.ã€F:model_pipeline1.pyâ€ L70-L92ã€‘ã€F:app.pyâ€ L1-L56ã€‘

6. **CI/CD Automation**  
   Jenkins orchestrates the end-to-end workflow: starting containers, running tests, executing training, verifying artefacts, and restarting the serving stack to deploy new models.ã€F:jenkinsfileâ€ L1-L206ã€‘

7. **Observability & Monitoring**  
   Metrics forwarded from `main1.py` are stored in Elasticsearch, visualised through Kibana, and complemented by MLflowâ€™s experiment UI for comprehensive monitoring.ã€F:main1.pyâ€ L24-L85ã€‘ã€F:docker-compose.ymlâ€ L4-L52ã€‘

## ğŸš€ Getting Started
1. **Clone & Create a Virtual Environment**
   ```bash
   git clone <repo-url>
   cd telecom-churn-prediction-mlops-pipeline
   make install
   ```

2. **Run the Training Pipeline Locally**
   ```bash
   make run
   ```
   This executes `main.py`, training a RandomForest model and logging the run to MLflow.ã€F:Makefileâ€ L15-L21ã€‘ã€F:main.pyâ€ L1-L31ã€‘

3. **Execute Unit Tests**
   ```bash
   make test
   ```
   The tests validate data preparation, training, evaluation, and model persistence flows.ã€F:Makefileâ€ L23-L34ã€‘ã€F:test_pipeline.pyâ€ L1-L38ã€‘

4. **Launch the FastAPI Service**
   ```bash
   make api
   ```
   The service loads the persisted model (`churn_model.pkl`) to expose `/predict`, `/healthcheck`, and `/retrain` endpoints.ã€F:Makefileâ€ L44-L52ã€‘ã€F:app.pyâ€ L1-L56ã€‘

5. **Start MLflow UI**
   ```bash
   make mlflow-ui
   ```
   Access the tracking dashboard at `http://127.0.0.1:5000` for local experiments.ã€F:Makefileâ€ L54-L58ã€‘

## ğŸ³ Containerised Deployment
1. Build and run the container locally:
   ```bash
   make docker-build
   make docker-run
   ```
   These commands wrap the FastAPI app into a Docker image and expose it on port 8005.ã€F:Makefileâ€ L60-L70ã€‘

2. For the full stack (MLflow, database, observability, API) run:
   ```bash
   docker-compose up -d
   ```
   This launches all services defined in `docker-compose.yml` with a shared network and persistent storage for PostgreSQL.ã€F:docker-compose.ymlâ€ L1-L52ã€‘

## ğŸ¤– Jenkins Automation
The `jenkinsfile` provisions Python dependencies, manages Docker services, runs Pytest, trains and logs the model via `main1.py`, validates the exported artefact, restarts the serving container, smoke-tests the API, and archives generated logs/models for traceability.ã€F:jenkinsfileâ€ L1-L206ã€‘ This pipeline ensures continuous delivery of updated churn models in a controlled manner.

## ğŸ“Š Monitoring & Observability
- **MLflow UI** â€“ Inspect runs, compare metrics, and download models logged by `main.py` and `main1.py`.
- **Elasticsearch & Kibana** â€“ `main1.py` pushes metrics into Elasticsearch (`mlflow-metrics` index), enabling Kibana dashboards for real-time performance visualisation.ã€F:main1.pyâ€ L24-L85ã€‘ã€F:docker-compose.ymlâ€ L4-L52ã€‘
- **Health Checks** â€“ The FastAPI `/healthcheck` endpoint reports the readiness of the serving model for external monitoring tools.ã€F:app.pyâ€ L38-L45ã€‘

## âœ… Testing Strategy
Pytest unit tests (`test_pipeline.py`) cover critical stages: data preparation, model training, metric evaluation, and Joblib persistence to guarantee pipeline stability before deployment.ã€F:test_pipeline.pyâ€ L1-L38ã€‘ Additional smoke tests are executed in Jenkins against the running API to confirm end-to-end functionality.ã€F:jenkinsfileâ€ L145-L194ã€‘

## ğŸ—ºï¸ Roadmap Ideas
- Add drift detection by monitoring feature distributions via Elasticsearch.
- Automate retraining triggers based on Kibana alerts or degraded metrics.
- Extend the FastAPI service with authentication and request logging.

---
Crafted for engineers who need a repeatable, observable, and production-ready telecom churn prediction workflow.
